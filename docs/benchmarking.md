# Benchmarking Intel OpenVINO Models

This document describes the benchmarking methodology used to compare FP16 and INT8 optimized models in the Nexus Ray framework.

## Overview

The Nexus Ray framework includes comprehensive benchmarking capabilities to demonstrate the performance benefits of IntelÂ® OpenVINOâ„¢ optimizations. Our benchmarks compare two versions of Mistral-7B:


## âš¡ Pre-Optimization vs. Post-Optimization Summary

The following table summarizes the performance jump from the baseline FP16 model to the OpenVINO-optimized INT8 model:

| Stage | Optimization | Throughput | Latency | RAM Usage |
|-------|--------------|------------|---------|-----------|
| **Pre-Optimization** | FP16 Baseline | ~1.2 tok/s | ~42.0s | 16.5 GB |
| **Post-Optimization** | **INT8 OpenVINO** | **~2.37 tok/s** | **~21.2s** | **8.8 GB** |
| **Improvement** | - | **ðŸš€ 1.98x** | **âš¡ 49.5%** | **ðŸ’¾ 46.6%** |

> [!IMPORTANT]
> This comparison demonstrates that Intel OpenVINO INT8 quantization not only approximately **doubles the throughput** but also brings the memory footprint down to a level that is manageable for standard consumer hardware and edge devices.

## Intel OpenVINOâ„¢ Optimization Benefits

### What is OpenVINO?

IntelÂ® OpenVINOâ„¢ (Open Visual Inference & Neural Network Optimization) is a toolkit for optimizing and deploying deep learning models. It provides:

1. **Model Quantization**: Reduces model size and increases inference speed
2. **CPU Optimization**: Leverages Intel CPU features (AVX, AVX2, AVX-512)
3. **Memory Efficiency**: Reduces RAM usage through INT8 quantization
4. **Latency Reduction**: Faster time-to-first-token and overall throughput

### Expected Performance Gains

Based on typical OpenVINO optimizations:

| Metric | Expected Improvement |
|--------|---------------------|
| **Throughput** | 2-3x faster (tokens/second) |
| **Latency** | 40-60% reduction |
| **Memory Usage** | 50-75% reduction |
| **Model Size** | ~50% smaller (14GB â†’ 7GB) |
| **Energy Efficiency** | 30-50% improvement |

## Benchmark Methodology

### Test Configuration

- **Hardware**: Intel CPU (specific model varies)
- **Framework**: Nexus Ray with OpenVINO integration
- **Models**: 
  - FP16: `OpenVINO/Mistral-7B-Instruct-v0.2-fp16-ov`
  - INT8: `OpenVINO/mistral-7b-instruct-v0.1-int8-ov`

### Test Prompts

We use domain-specific prompts from industrial/manufacturing scenarios:

1. **Safety Engineering**: Motor temperature and vibration analysis
2. **Compressed Air Systems**: Pressure drift diagnostics
3. **Electrical Systems**: Overload monitoring
4. **CNC Operations**: Vibration sensor anomaly detection
5. **Thermal Management**: Conveyor belt temperature monitoring
6. **Robotics**: Position sensor calibration

### Metrics Collected

For each prompt, we measure:

- **Tokens/Second**: Overall throughput
- **Latency**: Time to generate response
- **Time-to-First-Token (TTFT)**: Initial response time
- **CPU Usage**: Average CPU utilization
- **Memory Usage**: RAM consumed (GB)
- **Energy Estimate**: Approximate energy consumption (Joules)

## Running Benchmarks

### Prerequisites

```bash
# Install dependencies
pip install -r requirements.txt

# Ensure models are downloaded
python scripts/verify_downloads.py
```

### Execute Benchmark

```bash
# Run the benchmark script
python scripts/benchmark_models.py
```

This will:
1. Load both FP16 and INT8 models
2. Run inference on all test prompts
3. Collect performance metrics
4. Save results to `benchmark_results.json`

### Benchmark Script Location

- **Main Script**: [`scripts/benchmark_models.py`](../scripts/benchmark_models.py)
- **Download Helper**: [`scripts/download_models.bat`](../scripts/download_models.bat)
- **Verification**: [`scripts/verify_downloads.py`](../scripts/verify_downloads.py)

## Results Interpretation

### Understanding the Output

The `benchmark_results.json` file contains:

```json
[
  {
    "model": "mistral-7b-int8",
    "avg_tokens_per_second": 15.2,
    "avg_latency_per_request": 3.1,
    "load_time": 8.5,
    "memory_mb": 7200
  },
  {
    "model": "mistral-7b-fp16",
    "avg_tokens_per_second": 8.1,
    "avg_latency_per_request": 6.2,
    "load_time": 12.3,
    "memory_mb": 14500
  }
]
```

### Key Metrics Explained

**Tokens/Second (Higher is Better)**
- Measures how many tokens the model generates per second
- INT8 typically 2-3x faster than FP16

**Latency (Lower is Better)**
- Time taken to generate a complete response
- Directly impacts user experience

**Memory Usage (Lower is Better)**
- RAM consumed during inference
- INT8 uses ~50% less memory

**Load Time (Lower is Better)**
- Time to load model into memory
- Smaller INT8 models load faster

## Reproducing Results

### On Intel DevCloud

```bash
# SSH into DevCloud
ssh devcloud

# Navigate to project
cd Nexus_ray

# Run benchmark
python scripts/benchmark_models.py
```

### On Local Machine

```bash
# Ensure you have:
# - Python 3.12+
# - 32GB+ RAM (for FP16)
# - 16GB+ RAM (for INT8 only)

python scripts/benchmark_models.py
```

## Benchmark Limitations

### Known Constraints

1. **CPU-Only**: Benchmarks run on CPU (not GPU)
2. **Single-Threaded**: One prompt at a time
3. **No Batching**: Individual request processing
4. **Cache Effects**: Repeated prompts may show caching benefits

### Fair Comparison

Both models use:
- Same hardware
- Same prompts
- Same generation parameters (max_tokens=50)
- Same inference framework (OpenVINO)

## Integration with Nexus Ray

### Using Optimized Models

```python
from src.llm import get_llm

# Use INT8 optimized model
llm = get_llm("mistral-7b-ov")  # INT8 by default

# Generate
response = llm.generate("Analyze sensor data...")
```

### Configuration

Edit `src/core/config.py`:

```python
class LLMSettings(BaseModel):
    default_model: str = "mistral-7b-ov"  # INT8
    # Or: "mistral-7b-fp16" for FP16
    device: str = "CPU"
    quantization: str = "int8"
```

## Visualizing Results

### Creating Charts

Use the included Jupyter notebook (if available) or create custom visualizations:

```python
import json
import matplotlib.pyplot as plt

with open('benchmark_results.json') as f:
    results = json.load(f)

models = [r['model'] for r in results]
throughput = [r['avg_tokens_per_second'] for r in results]

plt.bar(models, throughput)
plt.ylabel('Tokens/Second')
plt.title('Model Throughput Comparison')
plt.show()
```

## References

- [Intel OpenVINO Documentation](https://docs.openvino.ai/)
- [Mistral-7B Model Card](https://huggingface.co/mistralai/Mistral-7B-v0.1)

## Contributing

To add new benchmark scenarios:

1. Edit `PROMPTS` list in `scripts/benchmark_models.py`
2. Run benchmark
3. Submit PR with updated results

## Support

For benchmark-related questions:
- Open an issue on GitHub
- Check [FAQ](./faq.md)
